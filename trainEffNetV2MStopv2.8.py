import os
import sys
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
from datetime import datetime

# === í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ íŒŒì¼ëª… ì¶œë ¥ ===
current_script = os.path.basename(sys.argv[0])
print(f"\nğŸš€ ì‹¤í–‰ íŒŒì¼ëª…: {current_script}\n")

# === ë°ì´í„° ê²½ë¡œ ì„¤ì • ===
BASE_DIR = "dataset_2000"
train_dir = os.path.join(BASE_DIR, "train")
val_dir = os.path.join(BASE_DIR, "val")
test_dir = os.path.join(BASE_DIR, "test")

# === íŒŒë¼ë¯¸í„° ì„¤ì • ===
BATCH_SIZE = 32
IMG_SIZE = (224, 224)
EPOCHS = 30

# === ë°ì´í„° ì¦ê°• (ê³¼ì í•© ë°©ì§€ + ì¼ë°˜í™” í–¥ìƒ) ===
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.25),
    layers.RandomZoom(0.3),
    layers.RandomContrast(0.3),
], name="data_augmentation")

# === ë°ì´í„°ì…‹ ë¡œë“œ ===
train_ds = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="int",
    shuffle=True
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    val_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="int",
    shuffle=False
)

test_ds = tf.keras.utils.image_dataset_from_directory(
    test_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="int",
    shuffle=False
)

# === class_names ìë™ ì €ì¥ (Djangoì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥) ===
class_names = train_ds.class_names
class_names_file = os.path.join(BASE_DIR, "class_names.txt")

with open(class_names_file, "w") as f:
    for name in class_names:
        f.write(name + "\n")

print(f"âœ… í´ë˜ìŠ¤ ëª©ë¡ì´ '{class_names_file}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"í´ë˜ìŠ¤ ê°œìˆ˜: {len(class_names)} â†’ {class_names}\n")

# === ë°ì´í„°ì…‹ ìµœì í™” ===
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))
train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)

# === í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ===
labels = np.concatenate([y for x, y in train_ds], axis=0)
class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(labels),
    y=labels
)
class_weights_dict = dict(enumerate(class_weights))
print("ğŸ“Š í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜:", class_weights_dict, "\n")

# === EfficientNetV2 ê¸°ë°˜ ëª¨ë¸ êµ¬ì„± ===
base_model = tf.keras.applications.EfficientNetV2M(
    include_top=False,
    input_shape=IMG_SIZE + (3,),
    weights="imagenet"
)
base_model.trainable = False  # ì „ì´í•™ìŠµ (Feature Extractor)

inputs = tf.keras.Input(shape=IMG_SIZE + (3,))
x = data_augmentation(inputs)
x = tf.keras.applications.efficientnet_v2.preprocess_input(x)
x = base_model(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(len(class_names), activation="softmax")(x)

model = models.Model(inputs, outputs)
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# === ì½œë°± ì„¤ì • ===
checkpoint_path = os.path.join("checkpoints", f"EffNetV2M_{datetime.now().strftime('%Y%m%d_%H%M%S')}.h5")
os.makedirs("checkpoints", exist_ok=True)

callbacks = [
    EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True),
    ReduceLROnPlateau(monitor="val_loss", factor=0.2, patience=3, min_lr=1e-6),
    ModelCheckpoint(checkpoint_path, save_best_only=True, monitor="val_loss")
]

# === ëª¨ë¸ í•™ìŠµ ===
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    class_weight=class_weights_dict,
    callbacks=callbacks
)

# === í…ŒìŠ¤íŠ¸ í‰ê°€ ===
test_loss, test_acc = model.evaluate(test_ds)
print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc * 100:.2f}% | í…ŒìŠ¤íŠ¸ ì†ì‹¤: {test_loss:.4f}")

# === ëª¨ë¸ ì €ì¥ ===
model.save("trained_model_EffNetV2M_v3.0.h5")
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: trained_model_EffNetV2M_v3.0.h5")
