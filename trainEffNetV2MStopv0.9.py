# =====================================
# ğŸ§  Colab ì„¸ì…˜ ìœ ì§€ ì½”ë“œ (ì—…ê·¸ë ˆì´ë“œ ë²„ì „)
# =====================================
from IPython.display import Javascript, display

def keep_colab_alive(interval_min=15):
    js_code = f"""
    async function ClickConnect() {{
        console.log("[KeepAlive] Colab ì„¸ì…˜ ìœ ì§€ ì¤‘...");
        const btn = document.querySelector("colab-connect-button") 
                 || document.querySelector("#connect") 
                 || document.querySelector("colab-toolbar-button#connect") 
                 || document.querySelector("colab-sessions-button") 
                 || document.querySelector("#top-toolbar colab-connect-button");
        if (btn) {{
            btn.click();
            console.log("âœ… ì—°ê²° ìœ ì§€ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ");
        }} else {{
            console.log("âš ï¸ Colab ì—°ê²° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (UI ë³€ê²½ ê°€ëŠ¥ì„±)");
        }}
    }}
    setInterval(ClickConnect, {interval_min} * 60 * 1000);
    console.log("âœ… Colab Keep-Alive í™œì„±í™”ë¨ â€” {interval_min}ë¶„ ê°„ê²©ìœ¼ë¡œ í´ë¦­ ì¤‘");
    """
    try:
        display(Javascript(js_code))
    except Exception as e:
        print("âš ï¸ Colab í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤. ì„¸ì…˜ ìœ ì§€ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë¬´ì‹œë©ë‹ˆë‹¤.")
        print(e)

keep_colab_alive(30)


# =====================================
# âš™ï¸ EfficientNetV2M Fine-Tuning í•™ìŠµ íŒŒì´í”„ë¼ì¸
# =====================================
import tensorflow as tf
from tensorflow.keras.applications import EfficientNetV2M
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import os

# ================================
# ì„¤ì •
# ================================
split_dir = "dataset_sp"  # train/val/test í¬í•¨ í´ë”
img_size = (224, 224)  # ì…ë ¥ í¬ê¸° ì •ì˜ë§Œ, resizeëŠ” í•˜ì§€ ì•ŠìŒ
batch_size = 32
epochs = 25
base_drive = Path("/content/drive/MyDrive")
model_save_path = "recycle_classifier_v2m_finetuned.keras"

# ================================
# ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
# ================================
train_dir = base_drive / split_dir / "train"
val_dir   = base_drive / split_dir / "val"
test_dir  = base_drive / split_dir / "test"

# ================================
# í´ë˜ìŠ¤ íƒìƒ‰
# ================================
classes = [d.name for d in train_dir.iterdir() if d.is_dir()]
class_indices = {cls: i for i, cls in enumerate(classes)}

# ================================
# í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ê°œìˆ˜
# ================================
class_counts = {cls: len(list((train_dir / cls).glob("*"))) for cls in classes}
max_count = max(class_counts.values())
print("í´ë˜ìŠ¤ë³„ ê°œìˆ˜:", class_counts)

# ================================
# train oversampling
# ================================
train_images, train_labels = [], []
for cls in classes:
    cls_path = train_dir / cls
    imgs = list(cls_path.glob("*"))
    if len(imgs) < max_count:
        imgs = imgs + np.random.choice(imgs, max_count - len(imgs)).tolist()
    labels = [class_indices[cls]] * len(imgs)
    train_images.extend([str(p) for p in imgs])  # ë¬¸ìì—´ ë³€í™˜
    train_labels.extend(labels)

# ì…”í”Œ
combined = list(zip(train_images, train_labels))
np.random.shuffle(combined)
train_images, train_labels = zip(*combined)

# ================================
# tf.data Dataset êµ¬ì„±
# ================================
def preprocess_image(file_path, label):
    img = tf.io.read_file(file_path)
    img = tf.image.decode_jpeg(img, channels=3)  # JPEGìœ¼ë¡œ ëª…ì‹œ (ì†ë„, ì•ˆì •ì„± â†‘)
    img = tf.cast(img, tf.float32) / 255.0       # ì •ê·œí™”
    return img, label

train_ds = tf.data.Dataset.from_tensor_slices((list(train_images), list(train_labels)))
train_ds = train_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
train_ds = train_ds.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)

def prepare_dataset(directory):
    images, labels = [], []
    for cls in classes:
        cls_path = Path(directory) / cls
        imgs = list(cls_path.glob("*"))
        images.extend([str(p) for p in imgs])  # ë¬¸ìì—´ ë³€í™˜
        labels.extend([class_indices[cls]] * len(imgs))
    ds = tf.data.Dataset.from_tensor_slices((images, labels))
    ds = ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)

val_ds = prepare_dataset(val_dir)
test_ds = prepare_dataset(test_dir)

# ================================
# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
# ================================
total = sum(class_counts.values())
class_weights = {i: total / count for i, (cls, count) in enumerate(class_counts.items())}
print("í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜:", class_weights)

# ================================
# ëª¨ë¸ êµ¬ì„± (EfficientNetV2M)
# ================================
base_model = EfficientNetV2M(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False  # 1ë‹¨ê³„: Transfer Learning

x = GlobalAveragePooling2D()(base_model.output)
x = Dropout(0.3)(x)
output = Dense(len(classes), activation="softmax")(x)
model = Model(inputs=base_model.input, outputs=output)

model.compile(
    optimizer=Adam(learning_rate=1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# ================================
# ì½œë°± ì„¤ì •
# ================================
callbacks = [
    EarlyStopping(monitor="val_accuracy", patience=5, restore_best_weights=True),
    ModelCheckpoint(model_save_path, monitor="val_accuracy", save_best_only=True),
    ReduceLROnPlateau(monitor="val_loss", factor=0.3, patience=3, min_lr=1e-6)
]

# ================================
# 1ë‹¨ê³„: Feature Extractor í•™ìŠµ
# ================================
print("\n===== [1ë‹¨ê³„] Transfer Learning í•™ìŠµ ì‹œì‘ =====")
history_stage1 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    class_weight=class_weights,
    callbacks=callbacks
)

# ================================
# 2ë‹¨ê³„: Fine-Tuning
# ================================
print("\n===== [2ë‹¨ê³„] Fine-Tuning í•™ìŠµ ì‹œì‘ =====")
for layer in base_model.layers[-40:]:  # ë§ˆì§€ë§‰ 40ê°œ layerë§Œ í•™ìŠµ í—ˆìš©
    layer.trainable = True

model.compile(
    optimizer=Adam(learning_rate=1e-5),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

history_stage2 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs,
    class_weight=class_weights,
    callbacks=callbacks
)

# ================================
# í•™ìŠµ ê²°ê³¼ ì‹œê°í™”
# ================================
def plot_history(hist1, hist2):
    acc = hist1.history["accuracy"] + hist2.history["accuracy"]
    val_acc = hist1.history["val_accuracy"] + hist2.history["val_accuracy"]
    loss = hist1.history["loss"] + hist2.history["loss"]
    val_loss = hist1.history["val_loss"] + hist2.history["val_loss"]

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(acc, label="train_acc")
    plt.plot(val_acc, label="val_acc")
    plt.title("Accuracy")
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(loss, label="train_loss")
    plt.plot(val_loss, label="val_loss")
    plt.title("Loss")
    plt.legend()
    plt.show()

plot_history(history_stage1, history_stage2)

# ================================
# ëª¨ë¸ ì €ì¥
# ================================
model.save(model_save_path, save_format="keras")
print("âœ… Fine-Tuned ëª¨ë¸ ì €ì¥ ì™„ë£Œ:", model_save_path)

# ================================
# ë¶„ë¥˜ í•¨ìˆ˜
# ================================
def classify_image(model, image_path, threshold=0.5):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.cast(img, tf.float32) / 255.0
    img = tf.expand_dims(img, axis=0)

    preds = model.predict(img)
    class_id = np.argmax(preds[0])
    confidence = preds[0][class_id]

    if confidence < threshold:
        return "ì¼ë°˜ì“°ë ˆê¸°"
    else:
        return classes[class_id]


# ì˜ˆì‹œ
# result = classify_image(model, "sample.jpg")
# print("ë¶„ë¥˜ ê²°ê³¼:", result)
