# ============================================================
# âœ… EfficientNetV2M í•™ìŠµ ìµœì¢… ì†ŒìŠ¤ ì½”ë“œ
# (Class Weight ì ìš©, steps_per_epoch ëª…ì‹œ, ì•ˆì •ì ì¸ 2ë‹¨ê³„ Fine-tuning)
# ============================================================

import os
import math
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks, metrics
from tensorflow.keras.applications import EfficientNetV2M
from tensorflow.keras.preprocessing import image_dataset_from_directory

# ============================================================
# 1. í™˜ê²½ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ============================================================
SEED = 42
BATCH_SIZE = 64
IMAGE_SIZE = (224, 224)
AUTOTUNE = tf.data.AUTOTUNE
EPOCHS_STAGE1 = 10
EPOCHS_STAGE2 = 10
LR_STAGE1 = 1e-4
LR_STAGE2 = 5e-6       # ğŸ’¡ Fine-tuning LRì€ ì•ˆì •ì„±ì„ ìœ„í•´ 5e-6ìœ¼ë¡œ ë‚®ì¶¤
FINE_TUNE_FRACTION = 0.2
PATIENCE_STAGE1 = 5    # Early Stopping Patience ìƒí–¥ ì¡°ì •
PATIENCE_STAGE2 = 6    # Early Stopping Patience ìƒí–¥ ì¡°ì •

train_dir = "/content/dataset_sp/train"
val_dir = "/content/dataset_sp/val"

# ğŸ’¡ í›ˆë ¨ ë°ì´í„°ì˜ ì‹¤ì œ í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ê°œìˆ˜ (ì œê³µëœ ë¡œê·¸ ê¸°ë°˜)
CLASS_COUNTS = np.array([
    13420, 13444, 23593, 19661, 10415, 19064, 10824, 24075, 6180,
    1922, 20107, 21424, 20706, 13699, 14598, 1040, 12100, 22527,
    19875, 2617, 20958, 13455, 19323, 23366, 15348, 1702, 22892,
    10860, 21714
])
total_train_samples = CLASS_COUNTS.sum()
num_classes = len(CLASS_COUNTS)

# ============================================================
# 2. Dataset ë¡œë“œ ë° ì¦ê°•
# ============================================================
train_ds = image_dataset_from_directory(
    train_dir,
    image_size=IMAGE_SIZE,
    batch_size=None,
    label_mode="categorical",
    shuffle=True,
    seed=SEED
)

val_ds = image_dataset_from_directory(
    val_dir,
    image_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="categorical",
    shuffle=False
)

class_names = train_ds.class_names

# ğŸ’¡ ë°ì´í„° ì¦ê°• í•¨ìˆ˜ (Augmentation ê°•í™”)
def augmentation_fn(image, label):
    # ì¢Œìš° ë°˜ì „
    image = tf.image.random_flip_left_right(image)
    # 90ë„ ë‹¨ìœ„ íšŒì „
    image = tf.image.rot90(image, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))
    # ë°ê¸° ì¡°ì ˆ
    image = tf.image.random_brightness(image, max_delta=0.2)
    return image, label

# í›ˆë ¨ Dataset íŒŒì´í”„ë¼ì¸
train_batched_ds = (
    train_ds
    .map(augmentation_fn, num_parallel_calls=AUTOTUNE)
    .shuffle(25000)  # ğŸ’¡ ì…”í”Œ ë²„í¼ í™•ëŒ€
    .batch(BATCH_SIZE)
    .prefetch(AUTOTUNE)
)

val_ds = val_ds.prefetch(AUTOTUNE)

# ============================================================
# 3. Class Weight ë° Steps per Epoch ê³„ì‚°
# ============================================================
# Class Weight ê³„ì‚° ë° Clipping
raw_weights = total_train_samples / (num_classes * CLASS_COUNTS)
clipped_weights = np.clip(raw_weights, a_min=None, a_max=10.0)
class_weights_dict = {i: float(w) for i, w in enumerate(clipped_weights)}

# ğŸ’¡ Steps per Epoch ê³„ì‚° (ì§„í–‰ ìƒíƒœ ë¡œê·¸ ì¶œë ¥ì„ ìœ„í•´ í•„ìˆ˜)
steps_per_epoch = math.ceil(total_train_samples / BATCH_SIZE)
print(f"ì´ í›ˆë ¨ ìƒ˜í”Œ: {total_train_samples}, ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}, Epochë‹¹ ìŠ¤í… ìˆ˜: {steps_per_epoch}")

# ============================================================
# 4. ëª¨ë¸ ì •ì˜ ë° Stage 1 í•™ìŠµ
# ============================================================
base_model = EfficientNetV2M(include_top=False, input_shape=IMAGE_SIZE + (3,), weights="imagenet")
base_model.trainable = False

inputs = layers.Input(shape=IMAGE_SIZE + (3,))
x = base_model(inputs, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(num_classes, activation="softmax")(x)
model = models.Model(inputs, outputs)

model.compile(
    optimizer=optimizers.Adam(learning_rate=LR_STAGE1),
    loss="categorical_crossentropy",
    metrics=["accuracy", metrics.TopKCategoricalAccuracy(k=3, name="top3"), metrics.TopKCategoricalAccuracy(k=5, name="top5")]
)

cb_stage1 = [
    callbacks.EarlyStopping(monitor="val_loss", patience=PATIENCE_STAGE1, restore_best_weights=True),
    callbacks.ModelCheckpoint("model_stage1_best.keras", monitor="val_loss", save_best_only=True, mode='min') # ğŸ’¡ í™•ì¥ì .keras ë³€ê²½
]

print("\n[Stage 1] ì „ì´ í•™ìŠµ ì‹œì‘...")
history1 = model.fit(
    train_batched_ds,
    validation_data=val_ds,
    epochs=EPOCHS_STAGE1,
    steps_per_epoch=steps_per_epoch, # âœ… steps_per_epoch ëª…ì‹œ
    class_weight=class_weights_dict,
    callbacks=cb_stage1
)


# ============================================================
# 5. Fine-tuning ë‹¨ê³„ (Stage 2)
# ============================================================

# âœ… 1ë‹¨ê³„ ìµœì  ê°€ì¤‘ì¹˜ ëª…ì‹œì  ë¡œë“œ (ì•ˆì •ì„± í™•ë³´)
try:
    # ğŸ’¡ .keras í™•ì¥ìë¡œ ë³€ê²½ëœ íŒŒì¼ ë¡œë“œ ì‹œë„
    model.load_weights("model_stage1_best.keras")
    print("âœ… Stage 1 ìµœì  ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ. Fine-tuning ì‹œì‘.")
except Exception as e:
    print(f"âŒ Stage 1 ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}. í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœë¡œ Fine-tuning ì§„í–‰.")

# ğŸ’¡ Fine-tuning ë²”ìœ„ ì„¤ì •: ì „ì²´ ë ˆì´ì–´ì˜ ìƒìœ„ 20%ë§Œ í•™ìŠµ
num_layers = len(base_model.layers)
fine_tune_at = int(num_layers * (1.0 - FINE_TUNE_FRACTION))

for i, layer in enumerate(base_model.layers):
    layer.trainable = (i >= fine_tune_at)

print(f"ì´ {num_layers}ê°œ ë ˆì´ì–´ ì¤‘ {fine_tune_at}ë²ˆì§¸ ({base_model.layers[fine_tune_at].name})ë¶€í„° Fine-tuning ì‹œì‘.")

# ë‚®ì€ LRë¡œ ì¬ì»´íŒŒì¼
model.compile(
    optimizer=optimizers.Adam(learning_rate=LR_STAGE2), # ğŸ’¡ 5e-6ë¡œ ë§¤ìš° ë‚®ì¶¤
    loss="categorical_crossentropy",
    metrics=["accuracy", metrics.TopKCategoricalAccuracy(k=3, name="top3"), metrics.TopKCategoricalAccuracy(k=5, name="top5")]
)

cb_stage2 = [
    callbacks.EarlyStopping(monitor="val_loss", patience=PATIENCE_STAGE2, restore_best_weights=True),
    callbacks.ModelCheckpoint("model_stage2_best.keras", monitor="val_loss", save_best_only=True, mode='min') # ğŸ’¡ í™•ì¥ì .keras ë³€ê²½
]

print("\n[Stage 2] Fine-tuning ì‹œì‘...")
history2 = model.fit(
    train_batched_ds,
    validation_data=val_ds,
    epochs=EPOCHS_STAGE2,
    steps_per_epoch=steps_per_epoch, # âœ… steps_per_epoch ëª…ì‹œ
    class_weight=class_weights_dict,
    callbacks=cb_stage2
)

# ============================================================
# 6. ìµœì¢… ëª¨ë¸ ì €ì¥
# ============================================================
# ğŸ’¡ ìµœì‹  Keras í‘œì¤€ì— ë”°ë¼ .keras í™•ì¥ìë¡œ ì €ì¥
model.save("model_final.keras", save_format="keras")
print("âœ… í•™ìŠµ ì™„ë£Œ ë° ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ (model_final.keras)")