# =====================================
# ğŸ§  Colab ì„¸ì…˜ ìœ ì§€ ì½”ë“œ (ì—…ê·¸ë ˆì´ë“œ ë²„ì „)
# =====================================

# from IPython.display import Javascript, display

# def keep_colab_alive(interval_min=15):
#     js_code = f"""
#     async function ClickConnect() {{
#         console.log("[KeepAlive] Colab ì„¸ì…˜ ìœ ì§€ ì¤‘...");
#         const btn = document.querySelector("colab-connect-button") 
#                  || document.querySelector("#connect") 
#                  || document.querySelector("colab-toolbar-button#connect") 
#                  || document.querySelector("colab-sessions-button") 
#                  || document.querySelector("#top-toolbar colab-connect-button");
#         if (btn) {{
#             btn.click();
#             console.log("âœ… ì—°ê²° ìœ ì§€ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ");
#         }} else {{
#             console.log("âš ï¸ Colab ì—°ê²° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (UI ë³€ê²½ ê°€ëŠ¥ì„±)");
#         }}
#     }}
#     setInterval(ClickConnect, {interval_min} * 60 * 1000);
#     console.log("âœ… Colab Keep-Alive í™œì„±í™”ë¨ â€” {interval_min}ë¶„ ê°„ê²©ìœ¼ë¡œ í´ë¦­ ì¤‘");
#     """
#     try:
#         display(Javascript(js_code))
#     except Exception as e:
#         print("âš ï¸ Colab í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤. ì„¸ì…˜ ìœ ì§€ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë¬´ì‹œë©ë‹ˆë‹¤.")
#         print(e)

# keep_colab_alive(30)


# ============================================================
# âœ… ì•ˆì •í™” í•™ìŠµìš© EfficientNetV2 ëª¨ë¸ (Google Drive ê²½ë¡œ ê¸°ë°˜)
# ============================================================

import os
import math
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing import image_dataset_from_directory
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# ============================================================
# 1. ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
# ============================================================
BASE_DIR = "dataset_sp"
# root = os.path.join("drive", "MyDrive", BASE_DIR)
root = "dataset_sp"
train_dir = os.path.join(root, "train")
val_dir = os.path.join(root, "val")
test_dir = os.path.join(root, "test")

# ============================================================
# 2. í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ============================================================
IMG_SIZE = (224, 224)
# BATCH_SIZE = 32
BATCH_SIZE = 8
EPOCHS = 30
SEED = 42

# ============================================================
# 3. ë°ì´í„°ì…‹ ë¡œë“œ (.repeat() í¬í•¨)
# ============================================================
# train_ds = image_dataset_from_directory(
#     train_dir,
#     image_size=IMG_SIZE,
#     batch_size=BATCH_SIZE,
#     label_mode="categorical",
#     shuffle=True,
#     seed=SEED
# )

train_ds = image_dataset_from_directory(
    train_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="categorical",
    shuffle=True,
    seed=SEED
)

# âš ï¸ class_namesëŠ” ì—¬ê¸°ì„œ ì¶”ì¶œí•´ì•¼ í•¨
class_names = train_ds.class_names
num_classes = len(class_names)
print(f"í´ë˜ìŠ¤ ê°œìˆ˜: {num_classes}")

# âœ… ì´í›„ì— ìºì‹œ, ì…”í”Œ, í”„ë¦¬í˜ì¹˜ ì ìš©
train_ds = train_ds.cache().shuffle(500).prefetch(1)

val_ds = image_dataset_from_directory(
    val_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="categorical",
    shuffle=False,
    seed=SEED
)

val_ds = val_ds.cache().prefetch(1)

test_ds = image_dataset_from_directory(
    test_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="categorical",
    shuffle=False
)
val_ds = test_ds.cache().prefetch(1)

# âœ… ì´ì œ generator ê³ ê°ˆ ë°©ì§€ìš© repeat ì ìš©
train_ds = train_ds.repeat()
val_ds = val_ds.repeat()  # âœ… validationë„ ë°˜ë³µ
# ============================================================
# 4. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• ë°ì´í„° ë³´ì •)
# ============================================================
# ê° í´ë”ë³„ ë°ì´í„° ê°œìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
all_labels = []
for class_index, class_name in enumerate(class_names):
    class_path = os.path.join(train_dir, class_name)
    count = len(os.listdir(class_path))
    all_labels += [class_index] * count

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(all_labels),
    y=all_labels
)
class_weight_dict = dict(enumerate(class_weights))
print("Class Weights:", class_weight_dict)

# ============================================================
# 5. Prefetch ìµœì í™”
# ============================================================
AUTOTUNE = tf.data.AUTOTUNE


# train_ds = train_ds.prefetch(AUTOTUNE)
# val_ds = val_ds.prefetch(AUTOTUNE)
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

test_ds = test_ds.prefetch(AUTOTUNE)

# ============================================================
# 6. ëª¨ë¸ ì •ì˜
# ============================================================
base_model = tf.keras.applications.EfficientNetV2B0(
    include_top=False,
    input_shape=IMG_SIZE + (3,),
    weights='imagenet'
)
base_model.trainable = False  # Stage 1: Feature extractorë§Œ ì‚¬ìš©

inputs = keras.Input(shape=IMG_SIZE + (3,))
x = tf.keras.applications.efficientnet_v2.preprocess_input(inputs)
x = base_model(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(num_classes, activation='softmax')(x)
model = keras.Model(inputs, outputs)

# ============================================================
# 7. ì»´íŒŒì¼
# ============================================================
optimizer = keras.optimizers.Adam(learning_rate=1e-3)
model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# ============================================================
# 8. ì½œë°± ì„¤ì •
# ============================================================
callbacks = [
    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)
]

# ============================================================
# 9. Steps ê³„ì‚° (ë°ì´í„° ê³ ê°ˆ ë°©ì§€)
# ============================================================
train_count = sum(len(files) for _, _, files in os.walk(train_dir))
val_count = sum(len(files) for _, _, files in os.walk(val_dir))
steps_per_epoch = math.ceil(train_count / BATCH_SIZE)
validation_steps = math.ceil(val_count / BATCH_SIZE)

print(f"Train samples: {train_count}, Val samples: {val_count}")
print(f"Steps per epoch: {steps_per_epoch}, Validation steps: {validation_steps}")

# ============================================================
# 10. 1ì°¨ í•™ìŠµ (Feature Extractor ë‹¨ê³„)
# ============================================================
history = model.fit(
    train_ds,
    epochs=EPOCHS,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_ds,
    validation_steps=validation_steps,
    class_weight=class_weight_dict,
    callbacks=callbacks
)

# ============================================================
# 11. 2ì°¨ í•™ìŠµ (Fine-tuning ë‹¨ê³„)
# ============================================================
base_model.trainable = True
for layer in base_model.layers[:200]:  # ì²˜ìŒ ëª‡ ì¸µì€ ê·¸ëŒ€ë¡œ ë‘ 
    layer.trainable = False

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history_finetune = model.fit(
    train_ds,
    epochs=EPOCHS,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_ds,
    validation_steps=validation_steps,
    class_weight=class_weight_dict,
    callbacks=callbacks
)

# ============================================================
# 12. ëª¨ë¸ ì €ì¥
# ============================================================
model_save_path = os.path.join(root, "efficientnetv2_stable_final.keras")
model.save(model_save_path , save_format="keras")
print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")

# ============================================================
# 13. í‰ê°€
# ============================================================
test_loss, test_acc = model.evaluate(test_ds)
print(f"âœ… Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}")


# ================================
# ë¶„ë¥˜ í•¨ìˆ˜
# ================================
# def classify_image(model, image_path, threshold=0.5):
#     img = tf.io.read_file(image_path)
#     img = tf.image.decode_jpeg(img, channels=3)
#     img = tf.cast(img, tf.float32) / 255.0
#     img = tf.expand_dims(img, axis=0)

#     preds = model.predict(img)
#     class_id = np.argmax(preds[0])
#     confidence = preds[0][class_id]

#     if confidence < threshold:
#         return "ì¼ë°˜ì“°ë ˆê¸°"
#     else:
#         return classes[class_id]

# def classify_image(model, image_path, classes, threshold=0.5):
#     """
#     í•™ìŠµìš© ì½”ë“œ(ì˜µì…˜1) ê¸°ë°˜ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì´ë¯¸ì§€ ë¶„ë¥˜
#     Args:
#         model: í•™ìŠµëœ Keras ëª¨ë¸
#         image_path: ë¶„ë¥˜í•  ì´ë¯¸ì§€ ê²½ë¡œ
#         classes: í•™ìŠµ ì‹œ ì‚¬ìš©ëœ í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸
#         threshold: confidence ì„ê³„ê°’
#     Returns:
#         class_name ë˜ëŠ” "ì¼ë°˜ì“°ë ˆê¸°"
#     """
#     # ì´ë¯¸ì§€ ì½ê¸° ë° ë””ì½”ë”© (JPEG, PNG ëª¨ë‘ ê°€ëŠ¥)
#     img = tf.io.read_file(image_path)
#     img = tf.image.decode_image(img, channels=3)
    
#     # í•™ìŠµìš© ì½”ë“œ ê¸°ì¤€: ì´ë¯¸ 224x224ì´ë¯€ë¡œ resize ë¶ˆí•„ìš”
#     img.set_shape([224, 224, 3])
    
#     # ì •ê·œí™”
#     img = tf.cast(img, tf.float32) / 255.0
    
#     # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
#     img = tf.expand_dims(img, axis=0)
    
#     # ì˜ˆì¸¡
#     preds = model.predict(img)
#     class_id = np.argmax(preds[0])
#     confidence = preds[0][class_id]
    
#     # threshold ê¸°ì¤€ íŒì •
#     if confidence < threshold:
#         return "ì¼ë°˜ì“°ë ˆê¸°"
#     else:
#         return classes[class_id], float(confidence)

# ì˜ˆì‹œ
# result = classify_image(model, "sample.jpg")
# print("ë¶„ë¥˜ ê²°ê³¼:", result)
